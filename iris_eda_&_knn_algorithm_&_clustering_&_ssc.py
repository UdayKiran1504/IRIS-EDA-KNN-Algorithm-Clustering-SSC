# -*- coding: utf-8 -*-
"""IRIS EDA & KNN Algorithm & Clustering & SSC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10vaNO8Q0MvJtq1c7QLXrOjW69Ul-11Mk
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import silhouette_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, f1_score

# Load the dataset
file_path = ('/content/IRIS - IRIS.csv')  # Replace with your file path if needed
data = pd.read_csv(file_path)

"""# **# --- Step 1: Exploratory Data Analysis (EDA) ---**"""

# Display basic info and summary statistics
print("Basic Info:")
print(data.info())

print("\nSummary Statistics:")
print(data.describe())

# --- Check for Missing Values ---
print("\nMissing Values in Each Column:")
missing_values = data.isnull().sum()
print(missing_values)

# --- Check for Duplicate Values ---
print("\nDuplicate Rows in Dataset:")
duplicate_rows = data.duplicated().sum()
print(f"Number of Duplicate Rows: {duplicate_rows}")

# Optionally, display the duplicate rows
if duplicate_rows > 0:
    print("\nDuplicate Rows:")
    print(data[data.duplicated()])

# Visualize pairwise relationships in the dataset (features)
sns.pairplot(data, hue='species')
plt.show()

# Visualize correlation matrix to check feature correlations
# First, drop the 'species' column since it's categorical
features = data.drop(columns=['species'])

# Compute the correlation matrix for the numerical features
corr_matrix = features.corr()

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""# **# --- Step 2: K-means Clustering ---**"""

# Preprocess the data
# Drop the 'species' column for clustering
features_scaled = StandardScaler().fit_transform(features)

# Apply K-means clustering with k=3 (for the 3 species in Iris dataset)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(features_scaled)

# Add the cluster labels to the original data for comparison
data['cluster'] = clusters

# Display the first few rows with cluster assignments
print("\nFirst few rows with cluster assignments:")
print(data.head())

# Display cluster counts
print("\nCluster counts:")
print(data['cluster'].value_counts())

# Evaluate the clustering performance using silhouette score (higher is better)
sil_score = silhouette_score(features_scaled, clusters)
print(f"\nSilhouette Score: {sil_score:.3f}")

# Visualize the clusters using the first two features (sepal length and sepal width)
plt.figure(figsize=(8, 6))
plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=clusters, cmap='viridis', marker='o', edgecolor='k')
plt.title("K-means Clustering Visualization")
plt.xlabel("Sepal Length (Scaled)")
plt.ylabel("Sepal Width (Scaled)")
plt.colorbar(label='Cluster Label')
plt.show()

"""# **# --- Step 3: KNN Algorithm ---**"""

# Splitting the data for KNN classification
X = data.drop(columns=['species', 'cluster'])
y = data['species']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply KNN Classification (using k=3)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Predict on the test set
y_pred = knn.predict(X_test)

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nKNN Classification Accuracy: {accuracy * 100:.2f}%")

# --- Confusion Matrix ---
# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
print("\nConfusion Matrix:")
print(conf_matrix)

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=y.unique(), yticklabels=y.unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# --- F1 Score ---
# Calculate the F1 score (for multi-class classification, it's averaged by default)
f1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' for multi-class
print(f"\nF1 Score (Weighted): {f1:.3f}")

# Visualize the classification results (only first two features for visualization)
# Convert predicted labels (if they're categorical) into numeric for visualization
y_pred_numeric = pd.factorize(y_pred)[0]  # Convert species names to numeric values

plt.figure(figsize=(8, 6))
plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred_numeric, cmap='viridis', marker='o', edgecolor='k')
plt.title("KNN Classification Visualization")
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.colorbar(label='Predicted Species')
plt.show()

"""# **# --- Step 4: SSC-like Approach (Subspace Clustering using PCA) ---**"""

# Apply PCA to reduce dimensionality to 2 components (mimicking SSC subspace technique)
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_scaled)

# Apply K-means clustering on the reduced subspace (PCA components)
kmeans_pca = KMeans(n_clusters=3, random_state=42)
pca_clusters = kmeans_pca.fit_predict(features_pca)

# Add the PCA cluster labels to the data for comparison
data['pca_cluster'] = pca_clusters

# Display the first few rows with PCA cluster assignments
print("\nFirst few rows with PCA-based cluster assignments:")
print(data[['species', 'pca_cluster']].head())

# Visualize the clusters based on the PCA components
plt.figure(figsize=(8, 6))
plt.scatter(features_pca[:, 0], features_pca[:, 1], c=pca_clusters, cmap='viridis', marker='o', edgecolor='k')
plt.title("PCA-based Subspace Clustering Visualization")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Cluster Label')
plt.show()

# Evaluate the clustering performance using silhouette score (higher is better)
sil_score_pca = silhouette_score(features_pca, pca_clusters)
print(f"\nSilhouette Score for PCA-based Clustering: {sil_score_pca:.3f}")